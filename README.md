# cogs118a-finalproject

Due on March 17, 2021 11:59PM CA timeNo late submission is possible without extremely unusual circumstances.Read this paper first: ​https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdfYour project will be to replicate  a part of the analysis done in the paper by Caruana &Niculescu-Mizil (hereafter referred to as CNM06). The undertaking is to investigate a few MLalgorithms and determine if any of them are better than others at binary classification tasks.You will write a report with >1,000 words (excluding references & appendices). The mainsections are: a) abstract, b) introduction, c) method, d) experiment, e) discussion, and f) at least3 references. Follow the paper format from a leading machine learning journal (e.g., Journal ofMachine Learning Research ​https://www.jmlr.org/format/authors-guide.html​) or conference (e.g.,NeurIPS ​click for LaTeX template​).Experiments & results:You will use at least 3 algorithms (truly different algos, not say kernel variations of the samealgo) on at least 4 different datasets. For each classifier and dataset combo, you will need to do5 trials. That’s 3x4x5 = 60 total trials.Each trial you will need to do a hyperparameter search to find the best settings to use for thealgorithm.  NB: this means you will likely be using different settings for each trial!  There aremany ways to do hyperparameter search which we will cover in class.  If you’re unsure whatmethod to use, just follow the procedure laid out in CNM06: randomly choose 5000 datasamples for your training set with replacement. Do 5-fold cross-validation on the training set toselect the hyperparameters via a systematic gridsearch of the parameter space. In CNM06section 2.1 they lay out the hyperparameter values they used in their search for each algorithm;you may use those for your search too.The exact number of train/validate cycles you do will depend on the hyperparameter search. Forexample if you are following the CNM06 method to investigate Logistic Regression it says: “trainboth unregularized and regularized models, varying the ridge (regularization) parameter byfactors of 10 from 10^-8 to 10^4.” That would be 14 total settings (including one whereregularization = 0) to try, yielding 14 hyperparameter settings * 5 folds which is 378 totaltrain/validate cycles just for a single trial of Logistic Regression. At the end of those 378train/validate cycles, you will select the hyperparameters settings that did best for the mean over
all 5 folds of that setting. Then you will train the model one more time on all 5000 training datasamples, and measure model performance on the test set (all the data in the dataset other thanthe 5000 random samples).To sum up: if you are doing the CNM06 logistic regression method you will be training60*378=22,680 times.  That will probably take somewhere between 10​1​ and 10​2 ​minutesbecause logistic regression is ​O​(n_samples * m_variables) time to train.  ​But for sloweralgorithms this can take hours or even days if you are exploring a large number ofhyperparameters.  So give yourself enough time to do this project, don’t wait until theweek before its due! ​ Also it can be worthwhile to time a single training/validation cycle andmultiply to predict how long it will take for a given algorithm to complete. Of course somehyperparameter settings can be slower to converge than others, so even this prediction may notbe accurate ​​ ̄\_(ツ)_/ ̄Model performance will be measured using at least 3 performance metrics that are in the range[0,1]. By default those metrics will be accuracy, F1 score, and AUC. However, depending on thedatasets and scenarios you take on it could be appropriate to try other metrics.  For instance forimbalanced datasets it has been argued that Matthew’s Correlation Coefficient is a better choicethan F1 (see ​this paper​).  That’s not to say that you should be using MCC; it’s to show you thatparticular needs given your task should affect your choice of metrics. And that a top class report(especially one from a large group) would take on some research in this respect.NB: DON’T run each trial 3 times to measure 3 metrics. You can either (a) run each trial once,save the predictions and true labels in arrays, and calculate 3 metrics after completing the trial;or (b) use sklearn’s convenience functions to calculate multiple metrics during thehyperparameter search.Your main results will be something similar to Tables 1-3 in CNM06:-Table 1 explaining the datasets-Table 2 giving the mean test set performance across trials for each algorithm/datasetcombo, with separate columns for each metric. There will also be a column for the meanperformance across metrics. This table should be annotated as in CNM06 usingboldface to show the best performing algorithm in each column and * to denote anon-significant difference between best algo and the others​1​.-Table 3 giving the mean test set performance across trials for each algorithm/metriccombo. Annotated as Table 2 to show best/non-significant differences from best.Secondary results you should report-An appendix table showing mean​ training​ set performance for the optimalhyperparameters on each of the dataset/algorithm combos and a discussion of thedifference between each algorithms’ training and test set performance-An appendix table with raw test set scores, not just the mean scores of the table1 Null hypothesis statistical testing is not going well powered at 5 trials. But it's the thought that counts Iguess?
-An appendix table with the p-values of the comparisons across algorithms in the differentmain matter tablesSecondary results you may wish to report (extra credit land):-An analysis of the time complexity of the algorithms you tried-A learning curve per algorithm/dataset combo: comparing test set performance for thebest hyperparameter choice as you vary the number of training samples or a givendataset-A heatmap-style plot of the validation performance vs hyperparameter setting for youralgorithms-Mean (over trials) ROC and PR curves for algorithm/dataset
